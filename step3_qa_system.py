#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Step 3: é—®ç­”ç³»ç»Ÿå‰åç«¯
åŸºäºStreamlitæ„å»ºçš„RAGé—®ç­”ç³»ç»Ÿï¼Œæ”¯æŒå¤šè½®å¯¹è¯å’Œä¸Šä¸‹æ–‡è®°å¿†
"""

import os
import streamlit as st
import faiss
import pickle
import numpy as np
from dotenv import load_dotenv
from openai import OpenAI
from typing import List, Dict, Any
import json
from datetime import datetime

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class RAGChatbot:
    """RAGèŠå¤©æœºå™¨äººç±»"""
    
    def __init__(self, vector_db_path: str = "./vector_db", 
                 chat_model: str = "gpt-4.1-2025-04-14",
                 embedding_model: str = "text-embedding-3-large"):
        """
        åˆå§‹åŒ–RAGèŠå¤©æœºå™¨äºº
        
        Args:
            vector_db_path (str): å‘é‡æ•°æ®åº“è·¯å¾„
            chat_model (str): å¯¹è¯æ¨¡å‹åç§°
            embedding_model (str): åµŒå…¥æ¨¡å‹åç§°
        """
        self.vector_db_path = vector_db_path
        self.chat_model = chat_model
        self.embedding_model = embedding_model
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        self.api_key = os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®OPENAI_API_KEY")
        
        self.client = OpenAI(api_key=self.api_key)
        
        # åŠ è½½å‘é‡æ•°æ®åº“
        self.load_vector_database()
        
        # åˆå§‹åŒ–å¯¹è¯å†å²
        self.conversation_history = []
    
    def load_vector_database(self):
        """åŠ è½½å‘é‡æ•°æ®åº“"""
        try:
            # åŠ è½½FAISSç´¢å¼•
            index_path = os.path.join(self.vector_db_path, "index.faiss")
            print("index_path:",index_path)
            self.index = faiss.read_index(index_path)
            
            # åŠ è½½æ–‡æœ¬å’Œå…ƒæ•°æ®
            with open(os.path.join(self.vector_db_path, "texts.pkl"), 'rb') as f:
                self.texts = pickle.load(f)
            
            with open(os.path.join(self.vector_db_path, "metadata.pkl"), 'rb') as f:
                self.metadata = pickle.load(f)
            
            # åŠ è½½é…ç½®
            with open(os.path.join(self.vector_db_path, "config.json"), 'r', encoding='utf-8') as f:
                self.config = json.load(f)
            
            print(f"âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ: {len(self.texts)} ä¸ªæ–‡æ¡£")
            
        except Exception as e:
            raise Exception(f"åŠ è½½å‘é‡æ•°æ®åº“å¤±è´¥: {str(e)}")
    
    def get_embedding(self, text: str) -> List[float]:
        """è·å–æ–‡æœ¬çš„å‘é‡è¡¨ç¤º"""
        response = self.client.embeddings.create(
            input=text,
            model=self.embedding_model
        )
        return response.data[0].embedding
    
    def search_similar_documents(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        æœç´¢ç›¸ä¼¼æ–‡æ¡£
        
        Args:
            query (str): æŸ¥è¯¢æ–‡æœ¬
            top_k (int): è¿”å›æ–‡æ¡£æ•°é‡
            
        Returns:
            List[Dict]: ç›¸ä¼¼æ–‡æ¡£åˆ—è¡¨
        """
        # è·å–æŸ¥è¯¢å‘é‡
        query_embedding = self.get_embedding(query)
        query_vector = np.array([query_embedding]).astype('float32')
        
        # æ ‡å‡†åŒ–æŸ¥è¯¢å‘é‡
        faiss.normalize_L2(query_vector)
        
        # æœç´¢ç›¸ä¼¼æ–‡æ¡£
        distances, indices = self.index.search(query_vector, top_k)
        
        results = []
        for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
            if idx != -1:  # æœ‰æ•ˆç´¢å¼•
                result = {
                    'text': self.texts[idx],
                    'metadata': self.metadata[idx],
                    'similarity': float(distance),
                    'rank': i + 1
                }
                results.append(result)
        
        return results
    
    def generate_context_from_documents(self, documents: List[Dict]) -> str:
        """ä»æ£€ç´¢åˆ°çš„æ–‡æ¡£ç”Ÿæˆä¸Šä¸‹æ–‡"""
        context_parts = []
        
        for doc in documents:
            source_info = ""
            if doc['metadata'].get('source') == 'markdown':
                section_title = doc['metadata'].get('section_title', 'N/A')
                section_level = doc['metadata'].get('section_level', 0)
                source_info = f"[æ–‡æ¡£ç« èŠ‚ - {section_title} (Level {section_level})]"
            
            context_parts.append(f"{source_info}\n{doc['text']}\n")
        
        return "\n---\n".join(context_parts)
    
    def generate_response(self, user_query: str, context: str, 
                         conversation_history: List[Dict] = None) -> str:
        """
        ç”Ÿæˆå›ç­”
        
        Args:
            user_query (str): ç”¨æˆ·é—®é¢˜
            context (str): æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            conversation_history (List[Dict]): å¯¹è¯å†å²
            
        Returns:
            str: ç”Ÿæˆçš„å›ç­”
        """
        # æ„å»ºç³»ç»Ÿæç¤º
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ä¼ä¸šå†…éƒ¨ææ•ˆã€‚ä½ ä¼šåŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹æ¥å›ç­”é—®é¢˜ã€‚

è¯·éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š
1. ä¸»è¦åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡å†…å®¹æ¥å›ç­”é—®é¢˜
2. å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®åœ°è¯´æ˜
3. ä¿æŒå›ç­”çš„å‡†ç¡®æ€§å’Œä¸“ä¸šæ€§  
4. å¦‚æœå¯èƒ½ï¼Œæä¾›å…·ä½“çš„å¼•ç”¨æˆ–å‡ºå¤„
5. å›ç­”è¦ç®€æ´æ˜äº†ï¼Œä¾¿äºç†è§£
6. æ”¯æŒä¸­æ–‡å¯¹è¯

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}
"""
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = [
            {"role": "system", "content": system_prompt.format(context=context)}
        ]
        
        # æ·»åŠ å¯¹è¯å†å²ï¼ˆä¿ç•™æœ€è¿‘5è½®å¯¹è¯ï¼‰
        if conversation_history:
            recent_history = conversation_history[-10:]  # æœ€è¿‘10æ¡æ¶ˆæ¯ï¼ˆ5è½®å¯¹è¯ï¼‰
            for msg in recent_history:
                messages.append(msg)
        
        # æ·»åŠ å½“å‰ç”¨æˆ·é—®é¢˜
        messages.append({"role": "user", "content": user_query})
        
        # ç”Ÿæˆå›ç­”
        try:
            response = self.client.chat.completions.create(
                model=self.chat_model,
                messages=messages,
                temperature=0.7,
                # max_tokens=1000
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            return f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}"
    
    def chat(self, user_query: str) -> Dict[str, Any]:
        """
        ä¸»è¦çš„èŠå¤©æ–¹æ³•
        
        Args:
            user_query (str): ç”¨æˆ·é—®é¢˜
            
        Returns:
            Dict[str, Any]: åŒ…å«å›ç­”å’Œç›¸å…³ä¿¡æ¯çš„å­—å…¸
        """
        # æœç´¢ç›¸å…³æ–‡æ¡£
        similar_docs = self.search_similar_documents(user_query, top_k=5)
        
        # ç”Ÿæˆä¸Šä¸‹æ–‡
        context = self.generate_context_from_documents(similar_docs)
        
        # ç”Ÿæˆå›ç­”
        response = self.generate_response(user_query, context, self.conversation_history)
        
        # æ›´æ–°å¯¹è¯å†å²
        self.conversation_history.append({"role": "user", "content": user_query})
        self.conversation_history.append({"role": "assistant", "content": response})
        
        return {
            'response': response,
            'context': context,
            'similar_documents': similar_docs,
            'timestamp': datetime.now().isoformat()
        }
    
    def clear_history(self):
        """æ¸…é™¤å¯¹è¯å†å²"""
        self.conversation_history = []

def get_available_databases():
    """è·å–å¯ç”¨çš„æ•°æ®åº“åˆ—è¡¨"""
    databases = []
    
    # é»˜è®¤æ•°æ®åº“
    if os.path.exists("./vector_db"):
        databases.append({"name": "é»˜è®¤æ•°æ®åº“", "path": "./vector_db"})
    
    # æ‰«æå‘½åæ•°æ®åº“
    for item in os.listdir("."):
        if item.startswith("vector_db_") and os.path.isdir(item):
            db_name = item.replace("vector_db_", "")
            databases.append({"name": db_name, "path": item})
    
    return databases

def init_streamlit_ui():
    """åˆå§‹åŒ–Streamlitç•Œé¢"""
    st.set_page_config(
        page_title="æ¯çº¿ç›¸å…³çŸ¥è¯†é—®ç­”Agentç³»ç»Ÿ",
        page_icon="ğŸ¤–",
        layout="wide"
    )
    
    st.title("ğŸ¤– æ¯çº¿ç›¸å…³çŸ¥è¯†é—®ç­”Agentç³»ç»Ÿ")
    st.markdown("### ä¼ä¸šå†…éƒ¨ææ•ˆçš„æ™ºèƒ½é—®ç­”åŠ©æ‰‹")
    
    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("ğŸ—„ï¸ æ•°æ®åº“é€‰æ‹©")
        
        # è·å–å¯ç”¨æ•°æ®åº“
        available_dbs = get_available_databases()
        
        if not available_dbs:
            st.error("âŒ æœªæ‰¾åˆ°å‘é‡æ•°æ®åº“!")
            st.info("è¯·å…ˆè¿è¡Œ Step 1 å’Œ Step 2 åˆ›å»ºæ•°æ®åº“")
            st.stop()
        
        # æ•°æ®åº“é€‰æ‹©å™¨
        db_options = [f"{db['name']} ({db['path']})" for db in available_dbs]
        selected_db_index = st.selectbox(
            "é€‰æ‹©æ•°æ®åº“",
            range(len(db_options)),
            format_func=lambda x: db_options[x],
            help="é€‰æ‹©è¦ä½¿ç”¨çš„å‘é‡æ•°æ®åº“"
        )
        
        selected_db = available_dbs[selected_db_index]
        selected_db_path = selected_db["path"]
        
        # æ˜¾ç¤ºæ•°æ®åº“ä¿¡æ¯
        if os.path.exists(os.path.join(selected_db_path, "config.json")):
            try:
                with open(os.path.join(selected_db_path, "config.json"), 'r') as f:
                    config = json.load(f)
                st.info(f"ğŸ“Š æ•°æ®åº“ä¿¡æ¯:\n- å‘é‡æ•°: {config.get('total_vectors', 'N/A')}\n- ç»´åº¦: {config.get('dimension', 'N/A')}")
            except:
                pass
        
        st.divider()
        
        st.header("âš™ï¸ ç³»ç»Ÿé…ç½®")
        
        # æ¨¡å‹é€‰æ‹©
        chat_model = st.selectbox(
            "å¯¹è¯æ¨¡å‹",
            ["gpt-4.1-2025-04-14"],
            index=0
        )
        
        # æ£€ç´¢è®¾ç½®
        top_k = st.slider("æ£€ç´¢æ–‡æ¡£æ•°é‡", 1, 10, 5)
        
        # æ¸…é™¤å†å²æŒ‰é’®
        if st.button("ğŸ—‘ï¸ æ¸…é™¤å¯¹è¯å†å²"):
            if 'chatbot' in st.session_state:
                st.session_state.chatbot.clear_history()
            if 'messages' in st.session_state:
                st.session_state.messages = []
            st.success("å¯¹è¯å†å²å·²æ¸…é™¤!")
            st.rerun()
        
        # ç³»ç»ŸçŠ¶æ€
        st.header("ğŸ“Š ç³»ç»ŸçŠ¶æ€")
        if 'chatbot' in st.session_state:
            chatbot = st.session_state.chatbot
            st.metric("å½“å‰æ•°æ®åº“", selected_db["name"])
            st.metric("ç´¢å¼•æ–‡æ¡£æ•°", len(chatbot.texts))
            st.metric("å¯¹è¯è½®æ•°", len(chatbot.conversation_history) // 2)
    
    return chat_model, top_k, selected_db_path

def main():
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–UI
    chat_model, top_k, selected_db_path = init_streamlit_ui()
    
    # æ£€æŸ¥APIå¯†é’¥
    if not os.getenv("OPENAI_API_KEY"):
        st.error("âŒ è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®OPENAI_API_KEY")
        st.stop()
    
    # æ£€æŸ¥å‘é‡æ•°æ®åº“
    if not os.path.exists(selected_db_path):
        st.error(f"âŒ æœªæ‰¾åˆ°å‘é‡æ•°æ®åº“: {selected_db_path}")
        st.stop()
    
    # æ•°æ®åº“è·¯å¾„æˆ–æ¨¡å‹å˜åŒ–æ—¶é‡æ–°åˆå§‹åŒ–èŠå¤©æœºå™¨äºº
    chatbot_key = f"chatbot_{selected_db_path}_{chat_model}"
    
    if chatbot_key not in st.session_state or st.session_state.get('current_db_path') != selected_db_path:
        try:
            with st.spinner("ğŸ”„ æ­£åœ¨åŠ è½½å‘é‡æ•°æ®åº“..."):
                st.session_state[chatbot_key] = RAGChatbot(
                    vector_db_path=selected_db_path,
                    chat_model=chat_model
                )
                st.session_state.chatbot = st.session_state[chatbot_key]
                st.session_state.current_db_path = selected_db_path
                
                # æ¸…é™¤æ¶ˆæ¯å†å²ï¼ˆåˆ‡æ¢æ•°æ®åº“æ—¶ï¼‰
                if 'messages' in st.session_state:
                    st.session_state.messages = []
                    
            st.success(f"âœ… å·²åŠ è½½æ•°æ®åº“: {selected_db_path}")
        except Exception as e:
            st.error(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}")
            st.stop()
    else:
        # ä½¿ç”¨å·²å­˜åœ¨çš„èŠå¤©æœºå™¨äºº
        st.session_state.chatbot = st.session_state[chatbot_key]
    
    # åˆå§‹åŒ–æ¶ˆæ¯å†å²
    if 'messages' not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "æ‚¨å¥½ï¼æˆ‘æ˜¯æ‚¨çš„æ™ºèƒ½é—®ç­”åŠ©æ‰‹,è¯·éšæ—¶æé—®ï¼"}
        ]
    
    # æ˜¾ç¤ºå¯¹è¯å†å²
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
        # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
        with st.chat_message("user"):
            st.markdown(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # ç”Ÿæˆå¹¶æ˜¾ç¤ºåŠ©æ‰‹å›ç­”
        with st.chat_message("assistant"):
            with st.spinner("ğŸ¤” æ­£åœ¨æ€è€ƒ..."):
                try:
                    # è·å–èŠå¤©æœºå™¨äººå›ç­”
                    result = st.session_state.chatbot.chat(prompt)
                    response = result['response']
                    
                    # æ˜¾ç¤ºå›ç­”
                    st.markdown(response)
                    
                    # æ˜¾ç¤ºç›¸å…³æ–‡æ¡£ï¼ˆå¯å±•å¼€ï¼‰
                    with st.expander("ğŸ“š ç›¸å…³æ–‡æ¡£ç« èŠ‚", expanded=False):
                        for i, doc in enumerate(result['similar_documents']):
                            st.markdown(f"**ç« èŠ‚ {i+1}** (ç›¸ä¼¼åº¦: {doc['similarity']:.3f})")
                            
                            # æ˜¾ç¤ºç« èŠ‚æ ‡é¢˜
                            metadata = doc['metadata']
                            if metadata.get('source') == 'markdown':
                                section_title = metadata.get('section_title', 'N/A')
                                section_level = metadata.get('section_level', 0)
                                st.markdown(f"**ç« èŠ‚æ ‡é¢˜**: {section_title}")
                                st.caption(f"ç« èŠ‚çº§åˆ«: Level {section_level}")
                            
                            # æ˜¾ç¤ºæ–‡æœ¬å†…å®¹
                            text_preview = doc['text'][:300] + "..." if len(doc['text']) > 300 else doc['text']
                            st.markdown(f"```\n{text_preview}\n```")
                            
                            st.divider()
                    
                except Exception as e:
                    response = f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}"
                    st.error(response)
        
        # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å†å²
        st.session_state.messages.append({"role": "assistant", "content": response})

if __name__ == "__main__":
    main() 